---
title: "Coursera Practical Machine Learning Project"
author: "Anna H"
date: "2025-03-05"
output: 
  html_document:
    output_file: "index.html"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE, 
                      cache = TRUE, 
                      # fig.width = 10, 
                      fig.align = "center")
options(scipen = 1e9)

packages <- c("caret", "ggplot2", "dplyr", "randomForest", "pROC", "scales")

# Load or install packages
load_or_install <- function(package) {
        if (!require(package, character.only = TRUE)) {
                install.packages(package)
                library(package, character.only = TRUE)
        }
}

# Apply the function to each package
sapply(packages, load_or_install)

```

### Introduction

This report details the development of a machine learning model for activity recognition using data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. The goal was to predict the manner in which they performed barbell lifts, classified into five categories (in the ***classe*** variable):  
  
<ul>
<li>A - exactly according to the specification</li>
<li>B - throwing the elbows to the front</li>
<li>C - lifting the dumbbell only halfway</li>
<li>D - lowering the dumbbell only halfway</li>
<li>E - throwing the hips to the front</li>
</ul>
### Data Preparation and Exploratory Analysis

```{r importData, include=FALSE}

# read in the data, exclude unnecessary fields that have to do with username or time windows
data <- read.csv("pml-training.csv", na.strings = c("", "NA", "#DIV/0!"),
                 colClasses = c(classe = "factor",
                                new_window = "NULL",
                                num_window = "NULL",
                                raw_timestamp_part_1 = "NULL",
                                raw_timestamp_part_2 = "NULL",
                                cvtd_timestamp = "NULL",
                                user_name = "NULL"))

# testData <- read.csv("pml-testing.csv", na.strings = c("", "NA", "#DIV/0!"),
#                  colClasses = c(new_window = "NULL",
#                                 num_window = "NULL",
#                                 raw_timestamp_part_1 = "NULL",
#                                 raw_timestamp_part_2 = "NULL",
#                                 cvtd_timestamp = "NULL",
#                                 user_name = "NULL"))
 
# select only important features (as per paper describing the data)
importantFeatures <- c(
        "roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt",
        "accel_belt_x", "accel_belt_y", "accel_belt_z",
        "gyros_belt_x", "gyros_belt_y", "gyros_belt_z",
        "magnet_belt_x", "magnet_belt_y", "magnet_belt_z",
        "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm",
        "magnet_arm_x", "magnet_arm_y", "magnet_arm_z",
        "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "total_accel_dumbbell",
        "accel_dumbbell_x", "accel_dumbbell_y", "accel_dumbbell_z",
        "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z",
        "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z",
        "roll_forearm", "pitch_forearm", "yaw_forearm", "total_accel_forearm",
        "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z"
)


data <- data %>% select(all_of(importantFeatures), "classe")
```

Our first step is to read in the data and clean it to make sure we only use the fields that are helpful in our predictions. The documentation for the original project that describes the data mentions 40 such fields out of the total 154. So, we will be using those 40 features as a starting point for our classification.

We will graph the distribution of the classes to get an better idea of the ***classe*** variable distribution in our data, and we will also check to make sure that are no rows left with missing information. And we see that there are `r sum(apply(data, 1, function(row) any(is.na(row))))` such rows.


```{r separateClasse, echo=FALSE}
# Create a data frame with the count of each classe
classeCount <- data %>% count(classe) %>% mutate(percentage = n / sum(n))

ggplot(classeCount, aes(x = classe, y = percentage)) +
        geom_bar(stat = "identity", fill = "lightblue", color = "lightblue3") +
        geom_text(aes(label = scales::percent(percentage, accuracy = 0.1)), vjust = -0.5) +
        scale_y_continuous(labels = scales::percent_format(accuracy=1), limits = c(0,0.3)) +
        labs(title = "Distribution of Classe Variables",
             x = "Classe",
             y = "Count") +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5))
```

```{r splitData, echo=FALSE}

# set the seed for various future calculations and reproducibility
set.seed(2543)

#split into training and testing sets

inTrain <- createDataPartition(y=data$classe, p=0.7, list=F)
training <- data[inTrain, ]
testing <- data[-inTrain, ]

training.backup <- training   # create a backup of the training data that still has "classe"
testing.backup <- testing   # create a backup of the testing data that still has "classe"

# remove "classe" variable to have a fully numeric data frame for manipulation and prediction
trainingClasse <- factor(training$classe)
training <- training %>% select(-last_col())
testingClasse <- factor(testing$classe)
testing <- testing %>% select(-last_col())

```

Next, the *classe* variable is separated from the dataset for ease of model training.  We then set a seed for reproducibility and split all of the data (the main dataset with the measurements and the one containing the classe information) into matching training and testing sets. The training dataset will be 70% of the original (`r format(nrow(training), big.mark=",", scientific=FALSE)` total observations) and the rest (`r format(nrow(testing), big.mark=",", scientific=FALSE)` total observations) will be left for model testing.

We will now take a look at the variability within our data set by looking at the coefficients of variation (CV) for each feature to see how large or small it is. First we see that there are several variables that have extreme (>1,000 or 10 times the mean) coefficients of variation. Suggesting extreme amounts of variation within the dataset.


``` {r highCV, echo=FALSE}
cv <- apply(training, 2, function(x) sd(x) / mean(x) * 100)
extremeCV <- which(abs(cv) > 1000)
print(paste(names(training)[extremeCV], collapse = ", "))
```


``` {r MadValues1, echo=FALSE}
madValues <- apply(training, 2, mad)
madValuesFrame <- data.frame(
    variable = names(madValues),
    mad = madValues
)

madOutlier <- madValuesFrame[which.max(madValuesFrame$mad), ]
madOutlierName <- madOutlier$variable
madOutlierValue <- madOutlier$mad
```

The median absolute deviation (MAD) values, which are less sensitive to outliers than CVs also shows quite a bit of variability as seen on the chart below (one extreme outlier has been left off - `r madOutlierName` with a value of `r sprintf("%.2f", madOutlierValue)`). 


``` {r MadValues2, echo=FALSE}
# Remove the outlier
madValuesFrame <- madValuesFrame[madValuesFrame$mad <= 250, ]

ggplot(madValuesFrame, aes(x = mad, y = reorder(variable, mad))) +
    geom_point(color = "lightblue3", fill = "lightblue", shape = 21, size = 3) +  
    geom_text(aes(label = sprintf("%s (%.2f)", variable, mad)), 
            color = "black", 
            size = 3, 
            family = "Arial", 
            hjust = 0,
            nudge_x = 5) +    
    labs(x = "MAD Value", y = "") +  
    # geom_text(aes(label = sprintf("%s (%.2f)", variable, mad)), hjust = -0.2, vjust = 0.5) +
    scale_x_continuous(limits = c(0, max(madValuesFrame$mad) * 1.2)) +
    labs(x = "MAD Value", y = "") +
    theme_minimal() +
    theme(
        axis.text.y = element_blank(),  # Remove y-axis labels
        ) +
  coord_cartesian(xlim = c(0, max(madValuesFrame$mad) * 1.3), clip = "off")

```

### Model Building

```{r trainModel, echo=FALSE}
# create a model using bootstrapping and k-fold cross-validation
if (!file.exists("savedResults.rda")) {

        # Set up k-fold cross-validation
        k <- 10
        folds <- createFolds(trainingClasse, k = k, list = TRUE, returnTrain = FALSE)
        
        # Define hyperparameter grid
        mtryValues <- c(sqrt(ncol(training)), ncol(training)/3, ncol(training)/2)
        ntreeValues <- c(100, 500, 1000)
        bootstrapSizes <- c(500, 1000, 1500)  # Add bootstrap sizes
        
        # Initialize matrix to store results
        results <- matrix(nrow = length(mtryValues) * length(ntreeValues) * length(bootstrapSizes),
                          ncol = 4)
        colnames(results) <- c("mtry", "ntree", "bootstrapSize", "accuracy")
        
        # Perform grid search with cross-validation
        row <- 1
        for (mtry in mtryValues) {
                for (ntree in ntreeValues) {
                    for (bootstrapSize in bootstrapSizes) {
                        cvAccuracy <- numeric(k)
                        for (i in 1:k) {
                                trainData <- training[-folds[[i]], ]
                                trainLabels <- trainingClasse[-folds[[i]]]
                                validData <- training[folds[[i]], ]
                                validLabels <- trainingClasse[folds[[i]]]
                                
                                
                                # Stratified sampling for bootstrapping
                                trainIndices <- createDataPartition(trainLabels, 
                                                 p = bootstrapSize/length(trainLabels), 
                                                 list = FALSE)
                                bootstrapData <- trainData[trainIndices, ]
                                bootstrapLabels <- trainLabels[trainIndices]
                                
                                
                               # use bootstrapping with randomForest
                                kModel <- randomForest(x = bootstrapData, y = bootstrapLabels, 
                                                       mtry = mtry, 
                                                       ntree = ntree, 
                                                       # sampsize = bootstrap_size, replace = T,
                                                       min.node.size = 5)
                                
                                predictions <- predict(kModel, validData)
                                cvAccuracy[i] <- mean(predictions == validLabels)
                        }
                        results[row,] <- c(mtry, ntree, bootstrapSize, mean(cvAccuracy))
                        row <- row + 1
                     }
                }        
        }
        
        # Find best hyperparameters
        bestParams <- results[which.max(results[,4]),]
        mtry <- bestParams["mtry"]
        ntree <- bestParams["ntree"]
        bootstrapValue <- bestParams["bootstrapSize"]
 
        # Use the best parameters in your existing train function
        # createDataPartition() is used to maintain the class distribution of the training data when sampling
        trainIndices <- createDataPartition(trainingClasse, 
                                      p = bootstrapValue / length(trainingClasse), 
                                      list = FALSE)
        bootstrapTraining <- training[trainIndices, ]
        bootstrapClasse <- trainingClasse[trainIndices]
        
        basicModel <- train(x = bootstrapTraining, 
               y = bootstrapClasse,
               method = "rf",
               trControl = trainControl(method = "cv", number = 10),
               metric = "Accuracy",
               tuneGrid = expand.grid(.mtry = mtry),
               ntree = ntree)
        

  save(results, bestParams, basicModel, mtry, ntree, bootstrapValue, file = "savedResults.rda")
  
} else {
# Load the saved results if the file exists
   load("savedResults.rda")
 }

```
#### Algorithm Selection

Since this a classification problem, we will be using a decision tree approach to get our *classe* prediction. However, a basic decision tree creates only one tree and may not capture the necessary relationships leading to a low level of accuracy. Therefore we will be utilizing an instance of bagging, which will allow us to train multiple models to get a better result. In this case, we chose a random forest method for the final model creation due its strong predictive performance and ability to capture non-linear relationships between variables without explicit specification of their relationships. Random forests are also known to help prevent overfitting, which is likely to happen when there's high variance, like in this data. This method also provides measures of variable importance that can potentially reduce the number of variables needed. 


#### Cross-Validation and Hyperparameter Tuning
An important step in training a good model is to select the appropriate parameters. A grid search with 10-fold k-fold cross-validation was done to tune the hyperparameters for the  
  
<ul>
<li>number of trees to grow (*ntree* is evaluated at values of 100, 500 and 1000</li>
<li>number of features to randomly sample at each tree split (*mtry* is evaluated at 6, 13 and 20)</li>
<li>sample tree sizes (bootstrap size is evaluated at 500, 1000 and 1500)</li>
</ul>
For each combination, stratified sampling for bootstrapping was used and the model's accuracy was evaluated using cross-validation. Unfortunately, this negatively affected the processing time. The best parameters were selected as follows:  
  
<ul>
<li>number of features - `r round(mtry)`</li>
<li>number of trees - `r ntree`</li>
<li>sample tree size - `r bootstrapValue`</li>
</ul>


```{r predictResults, echo=FALSE}

# create predictions based on the model (training)
basicPredictions <- predict(basicModel, training)
basicMatrix <- confusionMatrix(basicPredictions, trainingClasse, mode="everything", positive="1")


# take a sample of the testing subset b/c it's too large on its own
testingSize <- 1500  # adjust as needed
testingIndices <- sample(nrow(testing), testingSize)

# Create subsets for both testing features and classe
testingSample <- testing[testingIndices, ]
testingClasse <- testingClasse[testingIndices]

# Make predictions
testingPredictions <- predict(basicModel, testingSample)

# Create confusion matrix
basicTestingMatrix <- confusionMatrix(testingPredictions, testingClasse, 
                                      mode="everything", positive="1")


macroF1 <- mean(basicMatrix$byClass[, "F1"])
classCounts <- rowSums(basicMatrix$table)
weightedF1 <- sum(basicMatrix$byClass[, "F1"] * classCounts) / sum(classCounts)

testingMacroF1 <- mean(basicTestingMatrix$byClass[, "F1"])
testingClassCounts <- rowSums(basicTestingMatrix$table)
testingWeightedF1 <- sum(basicTestingMatrix$byClass[, "F1"] * classCounts) / sum(classCounts)

```

### Model Training and Evaluation
Next, those parameters were used to train a random forest model on a randomly selected part of the training set. 

```{r trainModelDisplay, eval=F}
        trainIndices <- createDataPartition(trainingClasse, 
                                      p = bootstrapValue / length(trainingClasse), 
                                      list = FALSE)
        bootstrapTraining <- training[trainIndices, ]
        bootstrapClasse <- trainingClasse[trainIndices]
        
        basicModel <- train(x = bootstrapTraining, 
               y = bootstrapClasse,
               method = "rf",
               trControl = trainControl(method = "cv", number = 10),
               metric = "Accuracy",
               tuneGrid = expand.grid(.mtry = mtry),
               ntree = ntree)

```


After making predictions the model is evaluated by looking at the confusion matrix.  

Overall, the model appear to have `r percent(basicMatrix$overall["Accuracy"], accuracy=0.01)` accuracy, which means that correctly classifies the vast majority of data. The Cohen's kappa for our model is `r round(basicMatrix$overall["Kappa"], 3)`, a value very close to 1. This means that the agreement between predicted and actual classes is not likely to have happened by chance.

The average of the F1 scores across the classes is `r percent(macroF1, accuracy = 0.01)`, which also suggests a high prediction quality that is also confirmed by the weighted-averaged of the F1 scores (`r percent(weightedF1, accuracy = 0.01)`).

The confusion matrix below shows that the model performs well across all classes, with high sensitivity, specificity, and F1 scores for each class. It does have slightly more difficulty identifying Class B (sensitivity - `r percent(basicMatrix$byClass["Class: B", "Recall"], accuracy=0.01)`) and has a high rate of false positives for Class C (precision - `r percent(basicMatrix$byClass["Class: C", "Precision"], accuracy=0.01)`.) It is likely that the issues for classes B and C were caused because both deal with the dumbbell being at the half-way point, making it hard to discern between the two classes from measurements alone. Even so, all the model quality measurements show that this is a strong model that should provide quality results.  


``` {r confusionMatrix, echo=F}
print(basicMatrix)
```

To confirm that the model is not overfitting, we will test its predictions on the testing data set that we separated from the training set earlier. With this never seen data we can confirm that the model is still performing well across all classes, with high accuracy (`r percent(basicTestingMatrix$overall["Accuracy"], accuracy=0.01)`), the difference of about `r percent(basicMatrix$overall["Accuracy"]-basicTestingMatrix$overall["Accuracy"], accuracy=0.01)` with the accuracy for the training set suggests there might be slight overfitting. The model also has a strong kappa (`r round(basicTestingMatrix$overall["Kappa"], 3)`) and the average F1 of `r percent(testingMacroF1, accuracy = 0.01)`, which still shows a strong agreement between predicted and actual classes and a good prediction quality.  
  
The model still struggles the most with correctly identifying Class B instances (with lowest sensitivity of all classes at `r percent(basicTestingMatrix$byClass["Class: B", "Recall"], accuracy=0.01)`) and is least confident in its Class C predictions (lowest precision at `r percent(basicTestingMatrix$byClass["Class: C", "Precision"], accuracy=0.01)`). Although these figures are slightly below the metrics from the training set, they are high enough to show that the model is still performing well even on unseen data.

When we create an ROC curve for both the training and testing set models we can also see that the area under the curve for each of the classes in both models is very close the the perfect measure of 1, confirming that the model is quite good at distinguishing between classes across various thresholds.


```{r ROCcurve, echo=FALSE, fig.width=12}

# Get probability predictions
probPredictions <- predict(basicModel, training, type = "prob")

# Create a list to store ROC objects
rocList <- list()

# Calculate ROC for each class
classes <- levels(trainingClasse)
for(i in seq_along(classes)) {
  rocList[[i]] <- roc(response = (trainingClasse == classes[i]),
                       predictor = probPredictions[,i])
}

# Get probability predictions for testing
testingPredictions <- predict(basicModel, testingSample, type = "prob")

# Create a list to store ROC objects
rocList2 <- list()

# Calculate ROC for each class
classes2 <- levels(testingClasse)
for(i in seq_along(classes2)) {
  rocList2[[i]] <- roc(response = (testingClasse == classes2[i]),
                        predictor = testingPredictions[,i])
}

# Function to format AUC values
format_auc <- function(classes, rocList, title) {
  output <- c(title, "")  # Add a blank line after the title
  for(i in seq_along(classes)) {
    output <- c(output, sprintf("AUC for class %s: %.3f", classes[i], auc(rocList[[i]])))
  }
  return(output)
}


# Generate formatted strings for both columns
col1 <- format_auc(classes, rocList, "Training AUC Values:")
col2 <- format_auc(classes2, rocList2, "Testing AUC Values:")

# Determine the maximum number of lines
max_lines <- max(length(col1), length(col2))

# Pad the shorter column with empty strings if necessary
col1 <- c(col1, rep("", max_lines - length(col1)))
col2 <- c(col2, rep("", max_lines - length(col2)))

layout(matrix(c(1,2,3,4), nrow=2, ncol=2, byrow=TRUE), heights=c(3,1))

# Plot ROC curves for training data
par(mar = c(5,4,4,2) + 0.1)
plot(rocList[[1]], col = rainbow(length(classes))[1], 
     main = "ROC Curves (Training)", lwd = 2)
for(i in 2:length(classes)) {
  plot(rocList[[i]], add = TRUE, col = rainbow(length(classes))[i], lwd = 2)
}
legend("bottomright", legend = classes,
       col = rainbow(length(classes)), lwd = 2)

# Plot ROC curves for testing data
par(mar = c(5,4,4,2) + 0.1)
plot(rocList2[[1]], col = rainbow(length(classes2))[1], 
     main = "ROC Curves (Testing)", lwd = 2)
for(i in 2:length(classes2)) {
  plot(rocList2[[i]], add = TRUE, col = rainbow(length(classes2))[i], lwd = 2)
}
legend("bottomright", legend = classes2,
       col = rainbow(length(classes2)), lwd = 2)

# Add AUC values for training data
par(mar = c(0,4,0,2) + 0.1)
plot.new()
text(x=0, y=1, labels=paste(col1, collapse="\n"), adj=c(0,1), family="mono", cex=1.0)

# Add AUC values for testing data
par(mar = c(0,4,0,2) + 0.1)
plot.new()
text(x=0, y=1, labels=paste(col2, collapse="\n"), adj=c(0,1), family="mono", cex=1.0)


# Reset the layout
par(mfrow = c(1, 1))
```

#### Expected Out-of-Sample Error

Given the test set accuracy of `r percent(basicTestingMatrix$overall["Accuracy"], accuracy=0.01)`, the expected out-of-sample error is approximately 1-accuracy = `r percent(1-basicTestingMatrix$overall["Accuracy"], accuracy=0.01)`. This estimate is based on the test set performance, which is a bit lower than the training set accuracy. However, it's important to note that this error rate might still be a little optimistic, and the performance on entirely new data could be slightly lower.
